\chapter{Transporte Óptimo de Masas}\label{chap:transporte-optimo-de-masas}  % MARK: Transporte Óptimo de Masas

En este capítulo se aborda el problema de transporte óptimo, la distancia de Wasserstein, y el problema de los baricentros de Wasserstein. Además, se presentan algunas propiedades de la distancia de Wasserstein que serán de utilidad en el desarrollo de este trabajo.
Es notable señalar que esta sección está basada en la Sección 1.1 del libro de Panaretos y Zemel \cite{panaretos2020invitation}, y en la Sección 2 del libro de Peyré y Cuturi \cite{peyre2019computational}.
% La notación, definiciones y explicaciones utilizadas están basadas principalmente en \cite[\S 3]{villani2009optimal}, \cite{peyre2019computational} y \cite{panaretos2020invitation}.

\section*{Notación}\label{sec:notacion}  % MARK: Notación

\FM[inline]{Agregar alguna intro para lo que es una medida de probabilidad? para aquellas personas que vienen de otras áreas?}
\begin{definition}
    \FM[inline]{Se podría dejar notación probabilística?}
    Se definen los siguientes espacios:
    \begin{itemize}
        \item Si $(\cX, \dist)$ es un espacio métrico, completo y separable, entonces se dice que este es un \emph{espacio Polaco}.
        \item Si $(\cX, \dist)$ es un espacio Polaco, $\ProbSpace[\cX]$ denotará al \emph{conjunto de medidas de probabilidad} en $\cX$, utilizando la $\sigma$-álgebra de Borel.
        \item Si $(\cX, \dist)$ es un espacio Polaco, $\ProbSpaceAC[\cX]$ denotará al \emph{conjunto de medidas de probabilidad absolutamente continuas} con respecto a una medida de referencia $\lambda$ (como por ejemplo, la de Lebesgue o la cuenta puntos), utilizando la $\sigma$-álgebra de Borel generada por $\dist$.
        \item $\ContSpace[\cX]$ denotará al \emph{conjunto de funciones continuas} en $\cX$.
        \item $\ContBoundedSpace[\cX]$ denotará al \emph{conjunto de funciones continuas y acotadas} en $\cX$.
        \item ${\Lip}_{k}(\cX)$ denotará al \emph{conjunto de funciones $k$-Lipschitz} en $\cX$. Mientras que se asumirá que $\Lip[\cX]$ denotará al \emph{conjunto de funciones $1$-Lipschitz} en $\cX$. \FM{Se podría agregar una definición de función Lipschitz?}
        \item Al conjunto de funciones de $A$ a $B$ se le denotará por $B^A$.
    \end{itemize}
\end{definition}

\section{El Problema de Transporte}\label{sec:el-problema-de-transporte}  % MARK: El Problema de Transporte

A lo largo de esta sección se presentan distintas formulaciones del problema de transporte, comenzando con sus orígenes en el trabajo de Monge \cite{monge1781memoire}, seguido de la formulación relajada de Kantorovich \cite{kantorovich1942translocation}, y concluyendo al destacar que, en realidad, ambos problemas son equivalentes, como lo establece el teorema de Brenier \cite{brenier1991polar}. Es notable señalar que esta sección está basada en la Sección 1.1 del libro de Panaretos y Zemel \cite{panaretos2020invitation}, y en la Sección 2 del libro de Peyré y Cuturi \cite{peyre2019computational}.

\subsection{El problema de Monge}  % MARK: El problema de Monge

Supongamos que hay un trabajador que debe de transportar una pila de arena a un pozo de igual volumen, situación que se puede ver representada en la Figura~\ref{fig:montanas-arena-pozo}. En esta situación, Monge \cite{monge1781memoire} en el 1781 se preguntó: ¿Cuál es la manera de transportar la arena al pozo con \emph{el menor esfuerzo} para el trabajador?, o dicho de otra manera, ¿Cuál es la forma \emph{óptima} de transportar la arena al pozo?

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/transporte/montanas-arena-pozo.png}
    \caption{Representación de la pila de arena, representada en rojo por la medida $\mu$, y el pozo, representada en azul por la medida $\nu$. Imagen obtenida de \cite{cuturi2017primer}.}
    \label{fig:montanas-arena-pozo}
\end{figure}

Utilizando términos matemáticos, este problema se formula de la siguiente manera: sea un espacio de arena $\cX$, un espacio de pozo $\cY$, y una función de costo $c: \cX \times \cY \to \R$, donde la función de costo representa el \emph{esfuerzo} necesario para transportar una unidad de arena del punto $x \in \cX$ a una posición en el pozo $y \in \cY$. La distribución de la arena se describe mediante una medida $\mu \in \ProbSpace[\cX]$, mientras que la forma del pozo se caracteriza mediante una medida $\nu \in \ProbSpace[\cY]$.

La \emph{decisión} sobre cómo transportar la arena se representa mediante una función $T: \cX \to \cY$, que asigna a cada punto $x \in \cX$ una posición $T(x) \in \cY$ en el pozo. El ``esfuerzo total'' que el trabajador debe realizar corresponde a ``sumar'' todos los costos asociados al transporte de la arena desde $x$ hasta la posición correspondiente en el pozo $T(x)$. Más formalmente, el \emph{costo total} de transportar la arena al pozo se obtiene integrando el costo $c$ sobre todo el espacio de arena $\cX$:
\begin{equation}\label{eq:costoTotalTransporteMonge}
    C(T) \eqdef \int_{\cX} c(x, T(x)) \dmu[x].
\end{equation}

Una primera propiedad que se debe exigir a la función $T$ es que preserve la masa total de la arena. Es decir, para cualquier conjunto $B \subseteq \cY$ que represente una región en el pozo con volumen $\nu(B)$, se requiere que el volumen de arena asignado a la región $B$ por la ``decisión'' $T$ sea exactamente el mismo volumen que se rellenará en $B$. La cantidad de arena que ocupará $B$ bajo la ``decisión'' $T$ se expresa como $\left\{ x \in \cX : T(x) \in B \right\} = T^{-1}(B)$, y por lo tanto, la condición de preservación de masa implica que $\mu(T^{-1}(B)) = \nu(B)$ para todo $B \subseteq \cY$.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/transporte/preservacion-masa.png}
    \caption{Representación de como la función $T$ ha de preservar la masa total de la arena. Imagen obtenida de \cite{cuturi2017primer}.}
    \label{fig:preservacion-masa}
\end{figure}

Esta condición se ilustra gráficamente en la Figura~\ref{fig:preservacion-masa} y se formula de manera más precisa mediante la definición del operador \textit{push-forward}:

\begin{definition}[Operador \textit{push-forward}]\label{def:operador-push-forward}
    Sea una función medible $T:\cX \to \cY$, se define el \emph{operador push-forward}  de $T$ como la aplicación $\Tpf:\ProbSpace[\cX] \to \ProbSpace[\cY]$ que satisface la siguiente relación:
    \begin{equation}
        \label{eq:pushForward}
        \Tpf \mu(B) \eqdef \mu (T^{-1}(B)) = \nu(B),\quad \forall B \subseteq \cY \text{ medible}.
    \end{equation}
\end{definition}

Ahora que se disponen de todas las herramientas matemáticas necesarias, se puede definir este problema como el de encontrar una función $T$, que representa la manera de transportar la arena al pozo, tal que minimice el costo total de transporte $C(T)$, sujeto a la condición de que la masa total de la arena sea preservada. De manera más formal:

\begin{definition}[Problema de Monge \cite{monge1781memoire}]
    Dadas dos medidas $\mu \in \ProbSpace[\cX]$ y $\nu \in \ProbSpace[\cY]$ y una función de coste $c : \cX \times \cY \to \R$, el \emph{problema de transporte óptimo de Monge} se define como el problema de encontrar una función medible $T: \cX \to \cY$ que minimice el costo total de transporte $C(T)$. Es decir, que minimice la siguiente expresión:
    \begin{equation}
        \label{eq:problemaTransporteMonge}
        \inf_{T: \Tpf \mu = \nu} \int_{\cX} c(x, T(x)) \dmu[x].
    \end{equation}
    A aquella función $T$ que resuelva este problema se le llamará \emph{función de transporte} o \emph{mapa de transporte}, y se denotará por $T_{\mu \to \nu}$, o simplemente $T$ si es que no existe confusión.
\end{definition}

El problema introducido por Monge \cite{monge1781memoire} es, en general, considerablemente difícil. Esto se debe principalmente a que el conjunto de mapas de transporte $\left\{ T : \Tpf \mu = \nu \right\}$ es intratable. Además, puede suceder que no exista ninguna solución, o en el caso de que exista, esta podría no ser única, como veremos en los siguientes ejemplos:

\begin{example}\label{ex:problemaDeMongeSinSolucion}
    Consideremos $\cX = \left\{ -1, 1 \right\}$, $\cY = \left\{ 0 \right\}$, con $\mu = \frac{1}{2} \delta_{-1} + \frac{1}{2} \delta_{1}$ y $\nu = \delta_0$. En este caso, la función de transporte óptima que minimiza \eqref{eq:problemaTransporteMonge} es
    \begin{align*}
        T_{\mu \to \nu}(-1) & = 0 & T_{\mu \to \nu}(1) & = 0.
    \end{align*}
    Sin embargo, es importante destacar que no existe un transporte óptimo $T_{\nu \to \mu}$ que transporte la masa de $\nu$ a $\mu$. Esto se debe a que la masa de $\nu$ está concentrada en un solo punto, mientras que la masa de $\mu$ está distribuida en dos puntos. No es posible ``dividir'' la masa debido a la naturaleza determinista de los mapas de transporte. Este ejemplo ilustra un caso en el que el problema de Monge no tiene solución.
\end{example}

\begin{example}\label{ex:problemaDeMongeMultipleSolucion}
    Consideremos ahora $\cX = \left\{ (1, 1), (-1, -1) \right\}$, $\cY = \left\{ (-1, 1), (1, -1) \right\}$, con $\mu = \frac{1}{2} \delta_{(1, 1)} + \frac{1}{2} \delta_{(-1, -1)}$ y $\nu = \frac{1}{2} \delta_{(-1, 1)} + \frac{1}{2} \delta_{(1, -1)}$. Este ejemplo correspondería a uno en que los puntos de $\cX$ y $\cY$ forman un cuadrado. En este escenario, se observa que existen dos funciones de transporte óptimo $T^1_{\mu\to\nu}$ y $T^2_{\mu\to\nu}$ que minimizan \eqref{eq:problemaTransporteMonge}. Estas funciones se expresan como:
    \begin{align*}
        T^1_{\mu\to\nu}(1, 1) & = (1, -1) & T^1_{\mu\to\nu}(-1, -1) & = (-1, 1)  \\
        T^2_{\mu\to\nu}(1, 1) & = (-1, 1) & T^2_{\mu\to\nu}(-1, -1) & = (1, -1).
    \end{align*}
    Este ejemplo muestra que, en ciertos casos, el problema de Monge puede tener más de una solución.
\end{example}



%   \begin{remark}
% 	  \label{remark:problemaTransporteMongeDiscreto}
% 	  Cuando las medidas $\mu$ y $\nu$ son discretas, es decir, se representan de la siguiente manera:
% 	  \begin{align}
% 		  \mu & = \sum_{i=1}^{n} a_i \delta_{x_i}, &
% 		  \nu & = \sum_{j=1}^{m} b_j \delta_{y_j},
% 	  \end{align}
% 	  donde $a \in \Simplex[n]$, $b \in \Simplex[m]$, $x_1,\ldots, x_n  \in \cX$, $y_1,\ldots, y_m  \in \cY$, entonces el problema de transporte óptimo se puede representar de la siguiente manera:
% 	  \begin{equation}
% 		  \label{eq:problemaTransporteDiscreto}
% 		  \inf_{T: T(x_i) = y_j} \sum_{i=0}^{n} a_i c(x_i, T(x_i)).
% 	  \end{equation}

\subsection{El problema de Kantorovich}  % MARK: El problema de Kantorovich

Como se pudo apreciar en los Ejemplos \ref*{ex:problemaDeMongeSinSolucion} y \ref*{ex:problemaDeMongeMultipleSolucion}, el problema de Monge no siempre tiene solución y, en caso de tenerla, puede que esta no sea única. Motivado por esto, Kantorovich \cite{kantorovich1942translocation} propuso una formulación relajada del problema de Monge.

La idea principal de Kantorovich es relajar la naturaleza determinista del mapa de transporte, es decir, el hecho de que la masa de un punto $x$ sea transportada a un único punto $T(x)$, como se ilustró en el Ejemplo~\ref{ex:problemaDeMongeSinSolucion}. En cambio, Kantorovich propone que la masa de un punto $x$ puede ser potencialmente transportada a múltiples destinos.

Para representar formalmente esta idea, consideremos una medida de probabilidad $\pi \in \ProbSpace[\cX \times \cY]$. En este contexto, la cantidad $\pi(A \times B)$ representaría la cantidad de arena transportada desde el conjunto $A \subseteq \cX$ hasta la región del pozo representada por el conjunto $B \subseteq \cY$. La masa total enviada desde $A$ sería $\pi(A \times \cY)$, y la masa total enviada a $B$ sería $\pi(\cX \times B)$. En este contexto, $\pi$ estaría preservando la masa total si, y solo si, se cumple que
\begin{align*}
    \pi(A \times \cY) & = \mu(A), \quad \forall A \subset \cX \text{ medible;} \\
    \pi(\cX \times B) & = \nu(B),\quad \forall B \subset \cY \text{ medible.}
\end{align*}
Las medidas $\pi$ que cumplen esta condición se conocen como \emph{couplings}, y se pueden definir de manera más formal de la siguiente manera:\FM{Quizás no sea necesaria la definición si lo definí en esta línea.}

\begin{definition}[Coupling]
    Sean $(\cX, \mu)$ y $(\cY, \nu)$ dos espacios de probabilidad. Un \emph{coupling} entre $\mu$ y $\nu$ es una medida de probabilidad $\pi \in \ProbSpace[\cX \times \cY]$ tal que sus proyecciones marginales sean $\mu$ y $\nu$, es decir, que cumpla que
    \begin{equation}
        \label{eq:coupling}
        \pi(A \times \cY) = \mu(A), \quad \pi(\cX \times B) = \nu(B), \quad \forall A \subseteq \cX, B \subseteq \cY \text{ medibles}.
    \end{equation}
    Al conjunto de couplings entre $\mu$ y $\nu$ se le denotará por $\Cpl(\mu, \nu)$. Usualmente se les llama a $\mu$ y $\nu$ como la primera y segunda \emph{distribución marginal}, o simplemente \emph{marginales} de $\pi$.
\end{definition}

Al igual que en el problema de Monge, se puede definir el costo total de transporte de un coupling $\pi \in \Cpl(\mu, \nu)$, utilizando una función de coste $c: \cX \times \cY \to \R$:
\begin{equation}
    C(\pi) = \int_{\cX \times \cY} c(x, y) \dpi (\dd x, \dd y).
\end{equation}

Y utilizando esta definición, se puede definir el problema de transporte óptimo de Kantorovich de forma análoga al problema de Monge:
\begin{definition}[Problema de Kantorovich \cite{kantorovich1942translocation}]
    Dadas dos medidas $\mu \in \ProbSpace[\cX]$ y $\nu \in \ProbSpace[\cY]$ y una función de coste $c: \cX \times \cY \to \R$, el \emph{problema de transporte óptimo de Kantorovich} se define como el problema de encontrar un coupling $\pi \in \Cpl(\mu, \nu)$ que minimice el costo total de transporte, es decir, que minimice la siguiente expresión:
    \begin{equation}
        \label{eq:problemaTransporteKantorovich}
        \inf_{\pi \in \Cpl(\mu, \nu)} \int_{\cX \times \cY} c(x, y) \dpi (\dd x, \dd y).
    \end{equation}
    Al conjunto de couplings que resuelven este problema se le llama \emph{planes de transporte} o \emph{couplings óptimos}, y un coupling que minimice este problema se denotará por $\pi_{\mu \to \nu}$, o simplemente $\pi$ si es que no existe confusión.
\end{definition}


En diferencia con el problema de Monge, el problema de Kantorovich siempre tiene solución, si es que $(\cX, \cY)$ son espacios compactos y $c$ es continuo. En efecto, $\Cpl(\mu, \nu)$ es compacto para la topología débil de las medidas,  $\pi \mapsto \int c\dd{\pi}$ es una función continua para esta topología, y la restricción $\pi\in\Cpl(\mu, \nu)$ es no vacía (la medida producto $\mu \otimes \nu$ pertenece a $\Cpl(\mu, \nu)$). Como se busca un ínfimo en un compacto de una función continua, se sigue que existe un coupling óptimo $\pi_{\mu \to \nu}$ que resuelve \eqref{eq:problemaTransporteKantorovich}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{img/transporte/coupling-example.png}
    \caption{Izquierda: coupling óptimo entre dos medidas 1-D continuas con densidad. El coupling está localizado a lo largo del grafo del mapa de transporte óptimo $(x, T(x))$. Derecha: coupling óptimo entre dos medidas discretas. El radio del disco negro es proporcional a la masa transportada en esa coordenada. Imagen obtenida de \cite{peyre2019computational}.
        \label{fig:coupling-example}}
\end{figure}

Continuando con los ejemplos \ref*{ex:problemaDeMongeSinSolucion} y \ref*{ex:problemaDeMongeMultipleSolucion}, podemos encontrar el coupling óptimo para cada uno de estos problemas:

\begin{example}
    Sean los espacios $\cX$ y $\cY$ y las medidas $\mu$ y $\nu$ como en el Ejemplo~\ref*{ex:problemaDeMongeSinSolucion}. En este caso, el coupling óptimo entre $\mu$ y $\nu$ corresponde a:
    \begin{align*}
        \pi_{\mu \to \nu}\left\{ (-1, 0) \right\} & = \frac{1}{2} & \pi_{\mu \to \nu}\left\{ (1, 0) \right\} & = \frac{1}{2}.
    \end{align*}
    Del mismo modo, el coupling óptimo entre $\nu$ y $\mu$ corresponde a:
    \begin{align*}
        \pi_{\nu \to \mu}\left\{ (0, -1) \right\} & = \frac{1}{2} & \pi_{\nu \to \mu}\left\{ (0, 1) \right\} & = \frac{1}{2}.
    \end{align*}

\end{example}

\begin{example}
    Del mismo modo, Sean los espacios $\cX$ y $\cY$ y las medidas $\mu$ y $\nu$ como en el Ejemplo~\ref*{ex:problemaDeMongeMultipleSolucion}. En este caso, el coupling óptimo entre $\mu$ y $\nu$ corresponde a:
    \begin{align*}
        \pi_{\mu \to \nu}\left\{ \qty\Big((1, 1), (-1, 1)) \right\}   & = \frac{1}{4} & \pi_{\mu \to \nu}\left\{ \qty\Big((1, 1), (1, -1)) \right\}   & = \frac{1}{4}  \\
        \pi_{\mu \to \nu}\left\{ \qty\Big((-1, -1), (-1, 1)) \right\} & = \frac{1}{4} & \pi_{\mu \to \nu}\left\{ \qty\Big((-1, -1), (1, -1)) \right\} & = \frac{1}{4}.
    \end{align*}

\end{example}

Para finalizar esta sección, una pregunta que surge naturalmente es: ¿Cuál es relación entre los problemas de Monge y Kantorovich? ¿Si encuentro una solución para el problema de Kantorovich, puedo tener una solución para el problema de Monge? La respuesta es que, bajo algunas condiciones particulares, estos dos problemas están estrechamente relacionados, como lo establece el teorema de Brenier \cite{brenier1991polar}:

\begin{theorem}[Brenier \cite{brenier1991polar}]
    Sea $\cX = \cY = \R^d$, $c(x, y) = \|x - y\|^2$, $\mu\in \ProbSpace[\cX] $ y $\nu\in \ProbSpace[\cY] $. Si al menos una de las dos medidas (que s.p.g. supondremos que es $\mu$) tiene densidad con respecto a la medida de Lebesgue, entonces el plan de transporte óptimo $\pi$ es único y esta soportado en el grafo $(x, T(x))$ de un mapa de transporte óptimo $T:\cX \to \cY$. Esto significa que el plan de transporte es $\pi = \pf{(\id, T)}\mu$, y cumple la siguiente ecuación:
    \begin{equation}
        \forall h \in \cC(\cX \times \cY),\quad \int_{\cX\times\cY}^{} h(x, y) \dpi (\dd x, \dd y) = \int_{\cX}^{} h(x, T(x)) \dmu[x].
    \end{equation}
    Más aún, el mapa de transporte óptimo $T$ está únicamente definido como el gradiente de una función convexa $\phi$, i.e. $T(x) = \grad \phi(x)$, donde $\phi$ es la única función convexa (salvo una constante aditiva) tal que $\pf{(\grad \phi)} \mu = \nu$.

    A aquellos mapas de transporte $\pi\in\Cpl(\mu, \nu)$ que se encuentran definidos por $\pi = \pf{(\id, T)} \mu$, para algún mapa de transporte $T$, se les llama \emph{mapas deterministas}, pues cumple con la propiedad de que si $(X, Y) \sim \pi$, entonces $Y$ queda determinado por $Y = T(X)$.
\end{theorem}

\begin{proof}
    Se puede encontrar una demostración de este teorema en \cite[p. 27]{peyre2019computational}
\end{proof}

\FM[inline]{Pensaba en agregar otra sección con la interpretación probabilista, como en \cite[\S 1.2]{panaretos2020invitation}}

\section{La Distancia y el Espacio de Wasserstein}\label{sec:la-distancia-y-el-espacio-de-Wasserstein}  % MARK: La Distancia y el Espacio de Wasserstein

Como se revisó en la sección anterior, cuando se considera el problema de Kantorovich, este problema usualmente tiene solución.
Si es que se considera $(\cX, \dist)$ un espacio métrico, se podría interpretar la distancia como una forma de representar el costo de transportar la masa de un punto $x$ a un punto $y$.
Pero una vez que se hace esto, naturalmente surge la pregunta: ¿Qué propiedades pueden surgir al considerar la distancia como una función de coste?
Esta sección responde a estas preguntas. Para ello, se empieza definiendo la distancia de Wasserstein, para luego revisar algunas de sus propiedades.
Esta sección estará basada en la Sección 6 del libro de Villani \cite{villani2009optimal}.
\FM{Incluir subsections para la distancia, espacio, y convergencia débil?}

\begin{definition}[La distancia de Wasserstein]\label{def:distanciaWasserstein}
    Sea $(\cX, \dist)$ un espacio Polaco y sea $p \geq 1$. Para dos medidas $\mu, \nu$ sobre $\cX$, la distancia de Wasserstein de orden $p$ entre $\mu$ y $\nu$ es definida por medio de la fórmula
    \begin{equation}
        \label{eq:distanciaWasserstein}
        \Wasserstein[p]{\mu}{\nu}  \eqdef \left( \inf_{\pi \in \Cpl(\mu, \nu)} \int_{\cX \times \cX} \dist(x, y)^{p} \dpi (\dd x, \dd y) \right)^{\frac{1}{p}}.
    \end{equation}

\end{definition}

\begin{example}
    $\Wasserstein[p]{\delta_x}{\delta_y} = \dist(x, y)$. Notemos que en este ejemplo, se puede interpretar que la distancia de Wasserstein metriza el ``esfuerzo'' de llevar la masa del punto $x$ al punto $y$, y que además, es independiente del valor de $p$.
\end{example}

Como el nombre de $W_p$ puede sugerir, esta es una distancia sobre medidas de probabilidad. Sin embargo, si esta es definida sobre todo el espacio $\ProbSpace[\cX]$, entonces puede tomar valores de $+\infty$, de forma que en estricto rigor, $W_p$ aún no puede ser considerada una distancia.
Para remediar este problema, se definirá un espacio de medidas de probabilidad en el que la distancia de Wasserstein tome valores finitos.

\begin{definition}[El espacio de Wasserstein]
    Con los mismos supuestos que en la Definición \ref{def:distanciaWasserstein}, se define el espacio de Wasserstein de orden $p$ por medio de
    \begin{equation}
        \WassersteinSpace[p]{\cX} \eqdef \left\{
        \mu \in \ProbSpace[\cX] \colon \int_{\cX} \dist(x, x_0)^{p} \dmu[x] < \infty
        \right\},
    \end{equation}
    para algún punto fijo $x_0 \in \cX$. De esta forma, $W_p$ define una distancia (finita) sobre $\WassersteinSpace[p]{\cX}$.
\end{definition}

En palabras simples, el espacio de Wasserstein de orden $p$ es el conjunto de medidas de probabilidad en $\cX$ cuyo momento de orden $p$ es finito.\FM{Será necesario este último comentario?}

En \cite[p. 94]{villani2009optimal} se puede encontrar una demostración de que $\WassersteinSpace[p]{\cX}$ satisface con los tres axiomas de una distancia. Es más, se puede demostrar que $\WassersteinSpace[p]{\cX}$ puede heredar varias propiedades del espacio base $\cX$, como lo presenta el siguiente teorema:
\begin{theorem}[Topología del espacio de Wasserstein \cite{villani2009optimal}]
    \label{thm:espacioWassersteinEsMetrico}
    Si $(\cX, \dist)$ es un espacio Polaco, entonces el espacio de Wasserstein $\WassersteinSpace[p]{\cX} $, metrizado por la distancia de Wasserstein $W_p$, es también un espacio Polaco.
\end{theorem}

\begin{proof}
    Revisar la demostración del Teorema 6.18 en \cite[p. 105]{villani2009optimal}
\end{proof}

\FM[inline]{Desde aquí en adelante, creo que se puede mover a otras secciones, si es que se necesita}

A partir de ahora, se asumirá que el espacio $\WassersteinSpace[p]{\cX} $ siempre estará equipado con su respectiva distancia $W_p$.\FM{Creo que esta línea no es del todo necesaria}


\begin{remark}
    A través de la desigualdad de Hölder, se puede demostrar que para $p \leq q$, se tiene que $\Wasserstein[p]{\mu}{\nu} \leq \Wasserstein[q]{\mu}{\nu}$, para toda $\mu, \nu \in \WassersteinSpace[p]{\cX}$. Y por tanto, las topologías inducidas por las distancias de Wasserstein se van encajonando.\FM{Esta observación no es tan necesaria para esta primera parte de preliminares}

    En particular, la distancia de Wasserstein de orden 1, es la más débil de todas. Como norma general, la distancia $W_1$  es la más flexible y fácil de acotar, mientras que la distancia $W_2$ posee mejores propiedades geométricas, pero es más difícil de trabajar.
\end{remark}

\FM[inline]{Toda la parte de la convergencia débil la moveré cuando pase a ver las WGANs}
Vista la distancia y el espacio de Wasserstein, se presentará una caracterización de convergencia en este espacio. Para ello, se definirá la convergencia débil entre medidas de probabilidad.

\begin{definition}[Convergencia Débil]
    Sea $(\cX, \dist)$ un espacio Polaco y sea $p \geq 1$. Se dice que una sucesión de medidas de probabilidad $(\mu_n)_{n \in \N} \subset \WassersteinSpace[p]{\cX} $ converge débilmente a $\mu \in \WassersteinSpace[p]{\cX}$ si
    \begin{equation}
        \forall \phi \in \ContBoundedSpace[\cX], \quad \int_{\cX} \phi(x) \dd{\mu_n(x)} \to \int_{\cX} \phi(x) \dmu[x].
    \end{equation}
    y lo denotaremos por $\mu_n \wto \mu$.
\end{definition}

\begin{note}
    Intuitivamente, que una sucesión de medidas de probabilidad converjan débilmente a una medida $\mu$ significa que es la forma ``más fácil'' que tiene la sucesión de converger a $\mu$.
\end{note}

\begin{theorem}[La Distancia de Wasserstein Metriza la Convergencia Débil]
    Sea $(\cX, \dist)$ un espacio Polaco y sea $p \geq 1$. Entonces, la distancia de Wasserstein $W_p$  metriza la convergencia débil en $\WassersteinSpace[p]{\cX}$.
\end{theorem}

\begin{remark}
    En otras palabras, si $(\mu_n)_{n\in\N}$ es una sucesión de medidas de probabilidad en $\WassersteinSpace[p]{\cX}$ y $\mu\in \WassersteinSpace[p]{\cX} $ otra medida, entonces $\mu_n \wto \mu$ si y sólo si $\Wasserstein[p]{\mu_n}{\mu} \to 0$.
\end{remark}

\begin{example}
    Consideremos las siguientes distancias y divergencias entre medidas de probabilidad:
    \begin{gather*}
        \TV{\mu}{\nu} \eqdef \sup_{A \subseteq \cX} \abs{\mu(A) - \nu(A)}, \\
        \KL{\mu}{\nu} \eqdef \int_{\cX} \log\left(\dv{\mu}{\nu}(x)\right) \dmu[x], \\
        \JS{\mu}{\nu} \eqdef \KL{\mu}{\frac{\mu + \nu}{2}} + \KL{\nu}{\frac{\mu + \nu}{2}},
    \end{gather*}
    donde la primera es la distancia total variación, la segunda es la divergencia de Kullback-Leibler, y la tercera es la divergencia de Jensen-Shannon.

    Si consideramos $\delta_\theta$ y $\delta_0$ medidas de Dirac centradas en $\theta$ y $0$ respectivamente, entonces se puede demostrar que
    \begin{align*}
        \Wasserstein[1]{\delta_\theta}{\delta_0} & = |\theta|                            &
        \TV{\delta_\theta}{\delta_0}             & = \begin{cases}
                                                         1 & \text{si } \theta \neq 0 \\
                                                         0 & \text{si } \theta = 0
                                                     \end{cases}          \\
        \KL{\delta_\theta}{\delta_0}             & = \begin{cases}
                                                         +\infty & \text{si } \theta \neq 0 \\
                                                         0       & \text{si } \theta = 0
                                                     \end{cases} &
        \JS{\delta_\theta}{\delta_0}             & = \begin{cases}
                                                         \log(2) & \text{si } \theta \neq 0 \\
                                                         0       & \text{si } \theta = 0
                                                     \end{cases}
    \end{align*}
    Entonces, si tomamos $\theta = \frac{1}{n} $ y dejamos que $n \to \infty$, se tiene que $\Wasserstein[1]{\delta_\theta}{\delta_0} \to 0$, pero el resto de distancias y divergencias no convergen a 0.
    Por tanto, se puede notar que la distancia de Wasserstein es la única que es capaz de distinguir entre medidas de probabilidad que no tienen soporte en el mismo punto, gracias a que metriza la convergencia débil.
\end{example}

\section{El Baricentro de Wasserstein}\label{sec:el-baricentro-de-Wasserstein-Bayesiano}  % MARK: El Baricentro de Wasserstein

Esta sección presenta el concepto de baricentro de Wasserstein, el cual es una generalización del concepto de promedio para medidas de probabilidad. Este concepto es clave para definir el baricentro de Wasserstein Bayesiano, el cual es el principal objeto de estudio de esta tesis. Para ello, se empieza revisando el concepto de media de Fréchet, luego se define el baricentro de Wasserstein, se revisan técnicas de cálculo de baricentros de población, y finalmente se define el baricentro de Wasserstein Bayesiano. La escritura de esta sección se basa en ejemplos de la Sección 3 de Panaretos y Zemel \cite{panaretos2020invitation} y en definiciones de la Sección 9.2 de Peyré y Cuturi \cite{peyre2019computational}.

\subsection{La Media de Fréchet}\label{ssec:la-media-de-Frechet}  % MARK: La Media de Fréchet
%   En esta sección se revisará el concepto de media de Fréchet, el cual es una generalización de la noción de promedio para espacios métricos. Este concepto será clave para definir el baricentro de Wasserstein.

\begin{definition}[Funcional y Media de Fréchet \cite{frechet1948elements}]
    Sea $(\cX, \dist)$ un espacio Polaco. Sean $(x_{i})_{i=1}^{n}\in\cX^n$ puntos en $\cX$ y sean $(\gamma_{i})_{i=1}^{n}\in\R_+^n$ los pesos asociados a esos puntos. Para cada $p\in\cX$, se define el \emph{funcional de Fréchet} por
    \begin{equation}
        \label{eq:funcionalFrechet}
        \Psi(p) \eqdef \sum_{i=1}^{n} \gamma_i \dist(p, x_i)^2.
    \end{equation}
    Y, en caso de que exista un punto $\bar x \in \cX$ que minimice el funcional $\Psi$, entonces este se definirá como la \emph{media de Fréchet} de los puntos $(x_{i})_{i=1}^{n}$. Es decir, es aquel punto tal que minimiza el siguiente problema:
    \begin{equation}
        \label{eq:mediaFrechet}
        \bar x \eqdef \argmin_{p \in \cX} \sum_{i=1}^{n} \gamma_i \dist(p, x_i)^2.
    \end{equation}
\end{definition}

\begin{example}\label{ex:baricentro-triangulo}
    Tomemos $x_1, x_2, x_3 \in \R^2$ tres puntos en el plano, formando un triángulo. Si se define el promedio (o el \textit{baricentro}, en el contexto de la geometría) de estos puntos por $\bar x = \frac{1}{3} (x_1 + x_2 + x_3)$, entonces se comprueba fácilmente que este es el único que minimiza el funcional de Fréchet:
    \begin{equation}
        F(p) = \frac{1}{3} \sum_{i=1}^{3} \|p - x_i\|^2,
    \end{equation}
    puesto que este funcional se descompone de la siguiente manera:
    \begin{equation}
        F(p) = F(\bar x) + \|p-\bar x\|^2.
    \end{equation}
    Con este ejemplo, se aprecia que la media de Fréchet generaliza la noción de promedio.
\end{example}

La razón por la que resulta interesante estudiar este concepto, es que únicamente utiliza nociones métricas, desligándose de nociones vectoriales. Como se vió en el Ejemplo~\ref{ex:baricentro-triangulo}, el promedio $\bar x$ utiliza conceptos vectoriales (suma y ponderación de vectores) mientras que la media de Fréchet utiliza nociones métricas (a través de la distancia $\dist(x, y) = \|x - y\|$), resultando en el mismo promedio.

Lo interesante de la media de Fréchet, es que este concepto es mucho más general, pues la media depende ahora de la distancia que se esté utilizando.


\subsection{El Baricentro de Wasserstein}\label{ssec:el-baricentro-de-Wasserstein}  % MARK: El Baricentro de Wasserstein

Como se vió en la sección anterior, la media de Fréchet permite definir una noción de promedio para espacios métricos. Además, el Teorema~\ref{thm:espacioWassersteinEsMetrico} establece que si $(\cX, \dist)$ es un espacio Polaco, entonces $(\WassersteinSpace[p]{\cX}, W_p)$ es un espacio Polaco, y en particular, un espacio métrico. Por tanto, se puede definir su respectivo ``promedio'' utilizando la media de Fréchet:

% MARK: Baricentro de Wasserstein
\begin{definition}[Baricentro de Wasserstein \cite{agueh2011barycenters}]\label{def:baricentroWasserstein}
    Sean $(\mu_{i})_{i=1}^{n}$ medidas en $\WassersteinSpace[p]{\cX} $ y sean $(\gamma_{i})_{i=1}^{n} \in \R_+^n$ sus pesos asociados. El \emph{baricentro de Wasserstein} se define por medio de
    \begin{equation}
        \bar \mu \eqdef \arginf_{\nu \in \WassersteinSpace[p]{\cX} } \sum_{i=1}^{n} \gamma_i \Wasserstein[p]{\nu}{\mu_i}^p.
    \end{equation}

\end{definition}

Un ejemplo de distintos baricentros, variando los pesos $(\gamma_i)_i$, se observa en la Figura~\ref{fig:baricentro-Wass-3d}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{img/transporte/baricentro-Wass-3d.png}
    \caption{Baricentro entre tres figuras en 3 dimensiones, donde se variaron los pesos $\gamma_i$ de forma bilineal. Imagen obtenida de \cite{solomon2015convolutional}.}
    \label{fig:baricentro-Wass-3d}
\end{figure}


% MARK: Interpolación geodésica
Cabe destacar que para el caso en donde se considera $n = 2$, el baricentro de Wasserstein recibe un nombre especial: \textit{interpolación geodésica de McCann}. En este caso, se consideran dos medidas $\mu_0, \mu_1 \in \WassersteinSpace[p]{\cX} $ y se define el baricentro de Wasserstein con pesos $\gamma = (1 - t, t)$, para $t \in [0, 1]$:
\begin{equation}\label{eq:interpolacion-geodesica}
    \mu_t \eqdef \arginf_{\mu \in \WassersteinSpace[p]{\cX} }  (1-t) \Wasserstein[p]{\mu_0}{\mu}^p + t \Wasserstein[p]{\mu}{\mu_1}^p .
\end{equation}


\FM[inline]{Se podría agregar el teorema que dice que el baricentro existe y es único, aunque no lo cacho mucho. Sé que es de Agueh y Carlier.}


Es posible generalizar aún más la noción de baricentro de Wasserstein a una colección infinita de medidas. Para esto, se considera una medida $\Gamma \in \ProbSpace[\ProbSpace ] $, que cumple el rol de los pesos $(\gamma_{i})_{i}$ en la definición anterior, lo que se formaliza en la siguiente definición:

\FM[inline]{Puede ser que en esta definición sea necesario incluir la definición utilizando el espacio de modelos como en el paper \cite{backhoff2022bayesian}}

% MARK: Baricentro de Población de Wasserstein
\begin{definition}[Baricentro de población de Wasserstein \cite{bigot2018characterization}]
    Sea $\Gamma \in \ProbSpace[\ProbSpace]$ una medida. El \emph{baricentro de población de Wasserstein} se define por medio de:
    \begin{equation}
        \label{eq:baricentroWassersteinGeneral}
        \bar \mu \eqdef \arginf_{\mu \in \ProbSpace} \int_{\ProbSpace} \Wasserstein[p]{\mu}{\nu}^p \dGamma[\nu].
    \end{equation}
\end{definition}

La razón por la que la definición anterior es una generalización, es que si se considera $\Gamma = \sum_{i=1}^{n} \gamma_i \delta_{\mu_i}$, entonces se recupera la primera definición. Además, cabe destacar que no es necesario especificar las medidas $(\mu_i)_{i=1}^{n}$, pues se encuentran implícitas en la medida $\Gamma$.

\begin{remark}
    Cabe destacar que en la definición anterior, el baricentro de Wasserstein es la medida que minimiza el promedio de las medidas generadas por $\Gamma$. De esta forma, la Ecuación \eqref{eq:baricentroWassersteinGeneral} se puede reescribir como:
    \begin{equation}
        \bar \mu \eqdef \arginf_{\mu \in \ProbSpace} \Big\{ \Exp_{\nu \sim \Gamma} \big[ \Wasserstein[p]{\mu}{\nu}^p \big] \Big\}.
    \end{equation}

\end{remark}

\subsection{El Descenso del Gradiente Estocástico sobre el Espacio de Wasserstein}\label{ssec:sgdw}  % MARK: SGDW El Descenso del Gradiente Estocástico sobre el Espacio de Wasserstein

Para calcular una estimación del baricentro de Wasserstein de una colección finita de medidas $\left\{ \mu_i \right\}_{i=1}^{n}$, existen múltiples algoritmos que se pueden utilizar. Por ejemplo, para el caso en que las medidas sean a soporte discreto, se puede deducir un problema de programación lineal \cite[ver Cap. 3]{peyre2019computational}, y resolver el problema de forma exacta \cite{bonneel2011displacement}, o utilizando una estimación de la solución por medio de métodos iterativos, como el algoritmo de Sinkhorn \cite{cuturi2013sinkhorn}. En el caso en que las medidas provengan de imágenes, se pueden utilizar métodos convolucionales, como en \cite{solomon2015convolutional} o en su versión mejorada e insesgada \cite{janati2020debiased}.

Sin embargo, para el caso en que la colección de medidas sean infinitas, como lo es en el caso del baricentro de población de Wasserstein, estos métodos no son aplicables. Por este motivo, en los trabajos \cite{rios2020contributions,backhoff2022stochastic,backhoff2022bayesian} proponen un algoritmo para encontrar este baricentro, por medio del Descenso del Gradiente Estocástico (SGD).

En los trabajos mencionados, se hacen los siguientes supuestos (que serán válidos sólo para esta sección de la tesis).

\begin{assumption}\label{assump:caso-particular-lebesgue}
    Se considera $p=2$, $\cX = \R^q$, $\dist$ la distancia Euclidiana, y $\lambda$ la medida de Lebesgue. Es más, con estos supuestos se tiene que $\Gamma \in \WassersteinSpace[2]{\WassersteinSpace[2,\mathrm{ac}]{\R^q} } $ y existe un espacio de modelos $\Models \subseteq \WassersteinSpace[2, \mathrm{ac}]{\R^q} $ para el cual $\Gamma$ es débilmente cerrado.
\end{assumption}

\begin{assumption}\label{assump:caso-particular-geodesicamente-convexo}
    $\Gamma$ tiene un espacio de modelos $W_2$-compacto: $K_\Gamma \subseteq \WassersteinSpace[2,\mathrm{ac}]{\R^q} $. Más aún, este conjunto es geodésicamente convexo: para cualesquiera $\mu_0, \mu_1 \in K_\Gamma$, la geodésica $\mu_t$ entre $\mu_0$ y $\mu_1$ está contenida en $K_\Gamma$.
\end{assumption}

\begin{assumption}\label{assump:caso-particular-esquema-paso-L1-L2}
    Para un esquema de paso\FM{Quizás podría definir qué es un esquema de paso.} $(\eta_k)_k \in [0, 1]^\N$\FM{Dejar que estoy anotando al conjunto de funciones de $A$ a $B$ por $B^A$.}, se asume que $\sum_{k=0}^{\infty} \eta_k = \infty$ y $\sum_{k=0}^{\infty} \eta_k^2 < \infty$.
\end{assumption}

\begin{definition}[Secuencia de SGDW \cite{backhoff2022stochastic,backhoff2022bayesian}]\label{def:sgdw}
    Dado $\mu_0 \in K_\Gamma$, $\tilde \mu_k \simiid \Gamma$ y $\eta_k > 0$ para $k \in\N$. Se define iterativamente la \textit{Secuencia de Descenso del Gradiente Estocástico sobre el Espacio de Wasserstein} (SGDW) por medio de:
    \begin{equation}
        \label{eq:sgdw}
        \mu_{k+1} \eqdef \pf{\qty\Big[
                (1 - \eta_k) \id + \eta_k T_{\mu_k \to \tilde \mu_k}
            ]} (\mu_k), \quad \text{para } k \in\N,
    \end{equation}
    donde $\eta_k \in [0, 1]$ es el tamaño de paso en la iteración $k$, y $T_{\mu_k \to \tilde \mu_k}$ es el transporte óptimo entre $\mu_k$ y $\tilde \mu_k$ como en la Definición~\ref{def:operador-push-forward}.
\end{definition}


\begin{remark}
    A pesar de que en la Definición~\ref{def:sgdw} se utilizó la notación de \textit{push-forward} (en donde se asume que $\mu_k$ y $\tilde \mu_k$ son absolutamente continuas con respecto a la medida de Lebesgue), basta con interpretar la notación de \textit{push-forward} como la interpolación geodésica entre $\mu_k$ y $\tilde \mu_k$, como se mencionó en \eqref{eq:interpolacion-geodesica}.

    Con esta idea en mente, en la iteración $k$ de la Secuencia de SGDW se interpreta como si se estuviera transportando el $\eta_k\%$ de la masa de $\mu_k$ a $\tilde \mu_k$. Si $(\eta_k)_k$ es decreciente, entonces cada vez se mantiene más la forma de $\mu_k$.
\end{remark}

\begin{remark}
    Gracias al supuesto de que $K_\Gamma$ es geodésicamente convexo, se tiene que $\mu_k \in K_\Gamma$ para toda $k \in\N$.
\end{remark}

El teorema principal que le da sentido a la definición anterior es el siguiente:

\begin{theorem}\label{thm:convergencia-sgdw}
    Se asumen los Supuestos~\ref{assump:caso-particular-lebesgue}, \ref{assump:caso-particular-geodesicamente-convexo}, \ref{assump:caso-particular-esquema-paso-L1-L2}, y que $\Gamma$ admite una única media de Karcher\FM{¿Será necesario incluir lo que es una media de Karcher? nunca lo ocupo directamente en el trabajo.}. Entonces la secuencia de SGDW $\left\{ \mu_k \right\}_k$ de la Ecuación~\eqref{eq:sgdw} converge c.s. al único 2-baricentro de Wasserstein $\bar \mu$ de $\Gamma$. Más aún, se tiene que $\bar \mu\in K_\Gamma$.
\end{theorem}

Además de la Secuencia de SGDW, en \cite{backhoff2022stochastic} se propone una versión de esta secuencia por lotes (o \textit{batched}, en inglés):

\begin{definition}[Secuencia de SGDW por lotes \cite{backhoff2022stochastic}]\label{def:bsgdw}
    Dado $\mu_0 \in K_\Gamma$, $\tilde \mu_k^{(1)}, \ldots, \tilde \mu_k^{(S_k)} \simiid \Gamma$ y $\eta_k > 0$ para $k \in\N$. Se define iterativamente la \textit{Secuencia de Descenso del Gradiente Estocástico sobre el Espacio de Wasserstein por lotes} (BSGDW) por medio de:
    \begin{equation}
        \label{eq:bsgdw}
        \mu_{k+1} \eqdef \pf{\qty[
                (1 - \eta_k) \id + \eta_k \frac{1}{S_k} \sum_{j=1}^{S_k} T_{\mu_k \to \tilde \mu_k^{(j)}}
            ]} (\mu_k) \quad \text{para } k \in\N,
    \end{equation}
    donde $\eta_k \in [0, 1]$ es el tamaño de paso, y $T_{\mu_k \to \tilde \mu_k^{(j)}}$ es el transporte óptimo entre $\mu_k$ y $\tilde \mu_k^{(j)}$ como en la Definición~\ref{def:operador-push-forward}.
\end{definition}

\begin{remark}\label{rem:bsgdw}
    En esta versión del SGDW por lotes, el operador \textit{push-forward} de la Ecuación~\eqref{eq:bsgdw} se interpreta como el baricentro de las medidas $(\mu_k, \tilde \mu_k^{(1)}, \ldots, \tilde \mu_k^{(S_k)})$ con pesos $(1 - \eta_k, \eta_k/S_k, \ldots, \eta_k/S_k)$.
\end{remark}

Los siguientes dos resultados dicen que la Secuencia de BSGDW aún converge, mientras que la segunda dice que los lotes ayudan a reducir la varianza de la estimación:

\begin{proposition}
    Bajo los supuestos del Teorema~\ref{thm:convergencia-sgdw}, la secuencia de BSGDW $\left\{ \mu_k \right\}_k$ de la Ecuación~\eqref{eq:bsgdw} converge c.s. al único 2-baricentro de Wasserstein $\bar \mu$ de $\Gamma$.
\end{proposition}

\begin{proposition}
    El estimador por lotes hace que la varianza integrada decrezca linealmente con el tamaño de los lotes.
\end{proposition}

Se termina esta sección presentando el algoritmo del BSGDW, pues es esta la versión más general. El siguiente algoritmo es una adaptación de la presentada en \cite{backhoff2022bayesian}, en donde se usa la Observación~\ref{rem:bsgdw}.
\begin{algorithm}[H]
    \caption{SGD sobre el Espacio de Wasserstein por lotes (BSGDW) \cite{backhoff2022bayesian}}
    \label{alg:bsgdw}
    \begin{algorithmic}[1]
        \Require Acceso a las muestras de $\Gamma(\dd \mu) \in \ProbSpace[\ProbSpace]$, un esquema de paso $(\eta_k)_k \in [0, 1]^\N$, y un esquema de lotes $(S_k)_k \in {\N_\ast}^\N$.
        \State{$k\gets0$}
        \State{Muestrear $\mu_0 \sim \Gamma$}
        \Repeat
        \State{Muestrear $\tilde \mu_k^{(1)}, \ldots, \tilde \mu_k^{(S_k)} \simiid \Gamma$}
        \State{$\gamma\gets \qty(1-\eta_k, \frac{\eta_k}{S_k}, \dots, \frac{\eta_k}{S_k})\in[0,1]^{S_{k+1}}$} \Comment{$\gamma$ es el vector de pesos.}
        \State{Definir $\mu_k$ como el baricentro de $(\mu_{k-1}, \tilde \mu_k^{(1)}, \ldots, \tilde \mu_k^{(S_k)})$ con pesos $\gamma$}
        \State{$k\gets k+1$}
        \Until{un criterio de detención ha sido alcanzado.}
        \State\Return $\mu_k$
    \end{algorithmic}
\end{algorithm}

El Algoritmo~\ref{alg:bsgdw} posee dos cuellos de botella: el primero es que se debe poder muestrear a partir de $\Gamma(\dd \mu)$, y el segundo es la capacidad de calcular el baricentro entre medidas. Para el primer problema, se pueden utilizar técnicas de Markov Chain Monte Carlo (MCMC) \cite{andrieu2003introduction,brooks2011handbook,goodman2010ensemble} para muestrear de $\Gamma$, o alguna otra técnica de muestreo; mientras que para el segundo, se pueden utilizar métodos iterativos como el algoritmo de Sinkhorn \cite{cuturi2013sinkhorn}, o en el caso de imágenes, métodos convolucionales \cite{solomon2015convolutional,janati2020debiased}.

Como parte del criterio de detención, se pueden utilizar algunos criterios clásicos (como por ejemplo, limitar el número máximo de iteraciones, o tener un tiempo de ejecución máximo), como también definir un criterio de convergencia basada en la distancia de Wasserstein entre iteraciones consecutivas. En particular, se puede utilizar el criterio de detención propuesto en \cite{backhoff2022bayesian}.



\subsection{El Baricentro de Wasserstein Bayesiano}\label{ssec:baricentro-Wasserstein-Bayesiano}  % MARK: El Baricentro de Wasserstein Bayesiano

En esta sección se define el baricentro de Wasserstein Bayesiano, el cual es una variante del baricentro de Wasserstein. La ventaja de este baricentro, es que permite encontrar una medida (el cuál, en este contexto, llamaremos \emph{modelo}) que mejor explique las muestras tomadas.
En esta sección se utilizará como referencia principal la sección 5 de la Tesis de G. Ríos \cite{rios2020contributions} y de la Sección~1 de J. Backhoff et al. \cite{backhoff2022bayesian}.

Se considera una muestra $\Data = \left(x_1,\ldots, x_n \right)$ que pertenecen a un espacio $\cX$ y un conjunto de modelos factibles
$\Models \subseteq \ProbSpace[\cX]$. Aprender un modelo (también conocido como la \emph{selección de un modelo}) utilizando los datos $\Data$  consiste en escoger un elemento $\mu$ de $\Models$ que \emph{mejor explique los datos}, si es que estos hubieran sido generados por $\mu$.

Para este objetivo, se adoptará un enfoque Bayesiano, el cuál provee de un marco probabilística para manejar la incertidumbre sobre los modelos. Por tanto, se empezará considerando una medida de probabilidad $\Pi \in \ProbSpace[\ProbSpace[\cX] ] $, entendida con una distribución a \textit{priori} sobre un espacio de modelos $\Models$. En particular, se tiene que el prior se encuentra soportada en el espacio de modelos $\Models$, es decir, $\Pi(\Models) = \Pi(\ProbSpaceAC[\cX] ) = 1$.

Para cada $n \in \N_\ast$, $\Pi$ induce una ley canónica $\bPi$ sobre $\cX^n\times \Models$, representando una ley conjunta de escoger un modelo $\mu$ de acuerdo a la ley $\Pi$, y una muestra i.i.d $\Data = (x_1,\ldots, x_n) \in \cX^n$ generadas por $\mu$. Esto es
\begin{align}
    \bPi(\dd x_1,\ldots, \dd x_n, \dd \mu)
     & \eqdef \dPi[\mu] \dmu[x_1] \cdots \dmu[x_n]                                      \\
     & = \dPi[\mu] \dlambda[x_1] \cdots \dlambda[x_n] \prod_{i=1}^{n} \rho_{\mu} (x_i),
\end{align}
donde $\rho_\mu \eqdef \dv{\mu}{\lambda}$ denota la derivada de Radon-Nikodym \cite{nikodym1930generalisation} de $\mu$ con respecto a la medida de referencia $\lambda$.
Considerando que $\bPi(\dd x_1,\ldots, \dd x_n, \dd \mu) = \Pi(\dd \mu) \bPi(\dd x_1,\ldots, \dd x_n \mid \mu)$, donde $\bPi(\dd x_1,\ldots, \dd x_n \mid \mu)$ es su respectivo kernel de transición, se deduce entonces que la ley sobre $\cX^n$ de los datos $\Data$, condicional al modelo $\mu$, está dado por
\begin{equation}
    \bPi(\dd x_1,\ldots, \dd x_n \mid \mu)
    \eqdef \dlambda[x_1] \cdots \dlambda[x_n] \prod_{i=1}^{n} \rho_{\mu} (x_i),
\end{equation}
donde se define la \emph{verosimilitud} como la densidad de $\bPi(\dd x_1,\ldots, \dd x_n \mid \mu)$ con respecto a $\lambda^{\otimes n}$, la cual se denotará por $\LikelihoodN[\mu]$:
\begin{equation}
    \LikelihoodN[\mu] \eqdef \rho_\bPi(x_1,\ldots, x_n \mid \mu) = \prod_{i=1}^n \rho_\mu(x_i)
\end{equation}
y la verosimilitud marginal, también conocido como la \emph{evidencia}, que se define por:
\begin{equation}
    \rho_\bPi(x_1,\ldots, x_n) \eqdef \int_{\Models} \LikelihoodN[\mu] \dPi[\mu].
\end{equation}

La \emph{densidad posterior} $\bPi(\dd\mu \mid x_1,\ldots, x_n )$, denotada por $\Pi_n(\dd\mu)$ por simplicidad, es un elemento de $\ProbSpace[\ProbSpace[\cX] ] $, y en virtud a la regla de Bayes, está dado por
\begin{equation}
    \label{eq:distribucionPosterior}
    \dPiN[\mu] = \dPi[\mu] \frac{\rho_\bPi(x_1,\ldots, x_n \mid \mu)}{\rho_\bPi(x_1,\ldots, x_n)} = \dPi[\mu] \frac{\LikelihoodN[\mu]}{\int_{\Models} \LikelihoodN[\nu] \dPi[\nu]}.
\end{equation}

La derivada de Radon-Nikodym de $\Pi_n(\dd \mu)$ con respecto a $\Pi(\dd \mu)$ es la \emph{verosimilitud normalizada} y se denotará por $\Lambda_n = \frac{\LikelihoodN[\mu]}{\int_{\Models} \LikelihoodN[\nu] \dPi[\nu]}$ , donde se puede notar que, dado que $\Models$ es un espacio de modelos para $\Pi$, y que $\Pi_n \ll \Pi$, entonces $\Models$ es un espacio de modelos para $\Pi_n$ también.

Ahora que se ha definido la distribución posterior sobre un conjunto de modelos $\Models$, podemos considerar que el modelo que mejor explique los datos $\Data$, correspondería al modelo que minimice el riesgo Bayesiano que utiliza como función de pérdida la distancia de Wasserstein:
\begin{equation}
    \label{eq:riesgoBayesiano}
    \mu_n = \arginf_{\mu \in \Models} \int_{\ProbSpace[\cX] } \Wasserstein[p]{\mu}{\nu}^p \dPiN[\nu].
\end{equation}
La definición anterior, corresponde justamente con la definición del baricentro de Wasserstein con respecto a $\Pi_n$, por lo que se puede definir el baricentro de Wasserstein Bayesiano de la siguiente manera:

\begin{definition}[Baricentro de Wasserstein Bayesiano \cite{backhoff2022bayesian}]
    \label{def:baricentroWassersteinBayesiano}
    Sea $\Pi \in \ProbSpace[\ProbSpace[\cX] ] $ una medida de probabilidad sobre un espacio de modelos $\Models \subseteq \ProbSpace[\cX]$. Sea $\Data = \left(x_1,\ldots, x_n \right) \in \cX^n$ una muestra de tamaño $n$. El \emph{baricentro de Wasserstein Bayesiano} se define como aquel modelo que minimice el siguiente problema:
    \begin{equation}
        \label{eq:baricentroWassersteinBayesiano}
        \bar \mu \eqdef \arginf_{\mu \in \Models} \int_{\ProbSpace[\cX] } \Wasserstein[p]{\mu}{\nu}^p \dPiN[\nu].
    \end{equation}
\end{definition}

\FM[inline]{Será necesario incluir una parte que describa métodos numéricos para calcular baricentros? Tenía pensado incluir el algoritmo del descenso del gradiente estocástico en espacio de Wasserstein, los algoritmos LP para el problema de Wasserstein, el de Sinkhorn, el Convolucional y el Debaised, que igual fueron métodos estudiados y utilizados.}
