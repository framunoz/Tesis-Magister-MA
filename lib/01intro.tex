\chapter{Introducción}

El tema principal de lo que trata este trabajo de tesis es acerca del \emph{Baricentro de Wasserstein Bayesiano}, concepto introducido por Gonzalo Ríos en el 2020 \cite{rios2020contributions}. Para poder explicar este concepto, se puede desglosar en tres partes: \textbf{baricentro}, \textbf{Wasserstein} y \textbf{Bayesiano}. A continuación se desarrolla cada uno de estos conceptos.

Se sabe que para cualquier triángulo formado por tres puntos $x_1, x_2, x_3 \in \R^d$, su \emph{baricentro} corresponde al promedio de los puntos, es decir, el punto $x_b$ tal que $x_b =\nolinebreak \frac{x_1 + x_2 + x_3}{3}$. Por tanto, el \textbf{baricentro} es sólo otra manera de llamar al \textbf{promedio}. Sin embargo, este baricentro también se puede calcular resolviendo el siguiente problema de optimización:
\begin{equation}\label{eq:baricentro-problema-optimizacion}
	x_b = \argmin_{x \in \R^d} \sum_{i=1}^3 \frac{1}{3} \norm{x - x_i}_2^2.
\end{equation}

En este contexto, la función definida por $\dist(x, y) \eqdef \norm{x - y}^2$ es una métrica en $\R^d$, y por tanto, el baricentro se puede interpretar como aquel punto que minimiza el promedio de las distancias con respecto a los puntos dados. En otras palabras, es aquel punto que, en promedio, intenta mantenerse lo más cerca posible de los demás puntos, siendo este un \textbf{representante} de todos ellos.

Si bien cambiar el cálculo del baricentro por un problema de optimización lo hace considerablemente más difícil, la ganancia de hacerlo radica en la generalidad del planteamiento. En efecto, el promedio utiliza nociones vectoriales (sumas y ponderación), mientras que el problema de optimización definido en \eqref{eq:baricentro-problema-optimizacion} utiliza únicamente nociones métricas (distancias), lo cual es una propiedad mucho más general. Esto significa que el baricentro depende \textbf{exclusivamente de la métrica que se esté utilizando}, lo que hace posible que \textbf{este cambie según la métrica empleada}.

Ahora que se ha explicado el concepto de \textbf{baricentro}, se procede a desarrollar la segunda idea del nombre: la parte de \textbf{Wasserstein}. Consideremos la situación en la que se tiene una montaña de tierra, y se desea mover esta tierra a un pozo, el cuál posee un volumen igual a la de la montaña. La pregunta es: ¿Cuál es la manera de mover la tierra de la montaña al pozo de la manera más eficiente? La respuesta a esta pregunta la da la teoría de \emph{transporte óptimo}, y a partir de esta rama es que se define la \emph{distancia de Wasserstein}.

En síntesis, la \emph{distancia de Wasserstein} es una métrica entre dos medidas de probabilidad: una medida $\mu\in \ProbSpace[\cX]$ que representa la montaña, y otra medida $\nu\in \ProbSpace[\cX]$ que representa el pozo. En este sentido, la distancia de Wasserstein \textbf{determina el esfuerzo promedio mínimo} que se necesita para mover la tierra de la montaña al pozo.

Teniendo en cuenta esta analogía, si es que la montaña estuviera más lejos del pozo, entonces la distancia de Wasserstein sería mayor. La propiedad de que esta distancia sea sensible a traslaciones espaciales (que no la cumplen otras métricas y divergencias) le provee a esta distancia múltiples propiedades interesantes.

Son estas propiedades que convierten a la distancia de Wasserstein en un método práctico para calcular baricentros. En donde, si se considera un conjunto de medidas de probabilidad $\{\mu_1, \ldots, \mu_n\}$, y denotando a la $p$-distancia de Wasserstein\footnote{La constante $p$ parametriza la distancia de forma similar a las $p$-normas, aunque para esta introducción, no es necesario entrar en detalles.} entre dos medidas $\mu$ y $\nu$ por $\Wasserstein[p]{\mu}{\nu}$, entonces el \emph{baricentro de Wasserstein} se define por
\begin{equation}\label{intro-wasserstein-barycenter}
	\bar\mu \eqdef \argmin_{\mu \in \ProbSpace[\cX]} \sum_{i=1}^n \gamma_i \Wasserstein[p]{\mu}{\mu_i}^p.
\end{equation}
donde $\left\{ \gamma_1, \ldots, \gamma_n \right\}$ es una secuencia de pesos no negativos que suman 1.

Cabe destacar que, en la definición del baricentro de Wasserstein, se está considerando un conjunto \textbf{finito} de medidas $\mu_1, \ldots, \mu_n$. Sin embargo, esta definición se puede generalizar a un conjunto \textbf{infinito} de medidas. Para esto, se considera una medida de probabilidad sobre medidas de probabilidad: una medida $\Gamma \in \ProbSpace[\ProbSpace[\cX]]$, que cumple el rol de los pesos $\gamma_i$ en la ecuación \eqref{intro-wasserstein-barycenter}. Con este enfoque, la sumatoria es reemplazada por una integral, y la definición del baricentro de Wasserstein se generaliza a
\begin{equation}
	\bar\mu = \argmin_{\mu \in \ProbSpace[\cX]} \int_{\ProbSpace[\cX]} \Wasserstein[p]{\mu}{\nu}^p \; {\Gamma(\dd \nu)}.
\end{equation}

Ahora que ya se conoce el concepto del \textbf{baricentro de Wasserstein}, se procede a explicar la componente \textbf{Bayesiana}. En el enfoque Bayesiano, es usual llamar por \emph{modelos} a las de medidas de probabilidad, donde al \emph{conjunto de modelos} se le denota por $\Models$.  Supongamos entonces que tenemos una muestra $\left\{ x_1, \ldots, x_n \right\}$ de $n$ datos que pertenecen a un espacio $\cX$.
A partir de esta muestra, se desea determinar un modelo $\bar{\mu}$ en $\Models\subseteq \ProbSpace[\cX]$ que mejor explique los datos, como si estos datos hubieran sido generados por $\bar{\mu}$.

Para esto, se considera la verosimilitud de un modelo $\mu$ y que lo denotaremos por $\LikelihoodN[\mu]$, y un prior sobre los modelos $\Pi(\dd \mu) \in \ProbSpace[\ProbSpace[\cX]]$. En virtud de la regla de Bayes, la \emph{medida posterior} se define por
\begin{equation}
	\Pi_n(\dd \mu) \eqdef \frac{\LikelihoodN[\mu]}{\int_{\ProbSpace[\cX]} \LikelihoodN[\nu] \Pi(\dd \nu)} \Pi(\dd \mu).
\end{equation}
De esta manera, la medida posterior le dará mayor peso a aquellas medidas que sean más verosímiles con respecto a los datos.

Así, el \emph{baricentro de Wasserstein Bayesiano} se define utilizando la medida posterior:
\begin{equation}
	\bar\mu \eqdef \argmin_{\mu \in \Models} \int_{\ProbSpace[\cX]} \Wasserstein[p]{\mu}{\nu}^p \; {\Pi_n(\dd \nu)}.
\end{equation}


Cabe mencionar que, gracias a las grandes propiedades que posee la distancia de Wasserstein y el baricentro de Wasserstein, es que ha sido utilizada en una gran variedad de aplicaciones:
para comparar los histogramas de colores \cite{rubner1998metric}, para comparar imágenes \cite{peleg1989unified}, para restaurar imágenes \cite{lellmann2014imaging}, para sintetizar texturas \cite{tartavel2016wasserstein}, en la clasificación de texto \cite{kusner2015word} y en particular,
en la función de pérdida para las redes generativas adversarias, para aliviar los problemas del desvanecimiento del gradiente y del modo de colapso \cite{arjovsky2017wasserstein}. Este último tema se trata en mayor profundidad más adelante en la tesis.

Sin embargo, desde la concepción del baricentro de Wasserstein Bayesiano, tan sólo se han realizado algunos experimentos con conjuntos de datos ``de juguete''\footnote{Se utiliza ``conjunto de datos de juguete'' para referirse al anglicismo \textit{toy datasets}.} (utilizando distribuciones normales, por ejemplo), para estudiar sus propiedades matemáticas y estadísticas, pero no se han visto aplicaciones en conjuntos de datos más complejos (como podría ser en imágenes).


Una razón de esto, es porque obtener la distribución posterior $\Pi_n(\dd \mu)$ puede ser infactible de calcular para conjuntos de datos finitos, incluso si estos son muy grandes. Otro motivo es que, en el caso en que se utiliza un conjunto infinito de modelos, es necesario utilizar el Descenso del Gradiente Estocástico sobre el Espacio de Wasserstein (en adelante, SGDW por sus siglas en inglés) \cite{backhoff2022stochastic}, el cuál es un algoritmo que, de la misma manera, es bastante reciente y no existen herramientas computacionales que lo implementen de forma general, y mucho menos para conjuntos de datos complejos.

Por este motivo, el objetivo general de este trabajo de tesis es la implementación eficiente de una librería en \emph{Python} para el cálculo del baricentro de Wasserstein Bayesiano sobre conjuntos de imágenes, utilizando el Descenso del Gradiente Estocástico sobre el Espacio de Wasserstein.

Dicho esto, los objetivos específicos son:
\begin{myitemize}
	\item \textbf{OE1}: Obtener aproximaciones de las medidas de probabilidad $\Gamma \in \ProbSpace[\ProbSpace[\cX]]$.
	\item \textbf{OE2}: Implementar el SGDW para una medida $\Gamma \in \ProbSpace[\ProbSpace[\cX]]$.
	\item \textbf{OE3}: Aproximar la medida posterior $\Pi_n(\dd \mu)$.
	\item \textbf{OE4}: Calcular el baricentro de Wasserstein Bayesiano.
\end{myitemize}
