\chapter{Introducción}

El tema principal de lo que tratará este trabajo de tesis es acerca del \emph{Baricentro de Wasserstein Bayesiano}, concepto introducido por Gonzalo Ríos en el 2020 \cite{rios2020contributions}. A pesar de que el nombre pueda parecer intimidante, este se puede desglosar en tres partes: \emph{baricentro}, \emph{Wasserstein} y \emph{Bayesiano}. A continuación se explicará cada uno de estos conceptos.

Del colegio se conoce que, para cualquier triángulo formado por tres puntos $x_1, x_2, x_3 \in \R^d$, su \emph{baricentro} corresponde al promedio de los puntos, es decir, el punto $x_b$ tal que $x_b =\nolinebreak \frac{x_1 + x_2 + x_3}{3}$. Por tanto, el \emph{baricentro} es sólo otra manera de llamar a lo que típicamente se conoce como el \emph{promedio}.

Sin embargo, este baricentro también se puede calcular resolviendo el siguiente problema de optimización:
\begin{equation}\label{eq:baricentro-problema-optimizacion}
    x_b = \argmin_{x \in \R^d} \sum_{i=1}^3 \frac{1}{3} \norm{x - x_i}_2^2.
\end{equation}
Como es bien sabido, la función definida por $\dist(x, y) \eqdef \norm{x - y}^2$ es una métrica en $\R^d$, y por tanto, el baricentro se puede interpretar como aquel punto que minimiza el promedio de las distancias con respecto a los puntos dados. En otras palabras, es aquel punto que, en promedio, intenta mantenerse lo más cerca posible de los demás puntos, siendo este un \emph{representante} de todos ellos.

Ahora bien, el lector se puede preguntar:
\begin{quotation}
    \textit{¿De verdad fue necesario haber cambiado el promedio por un problema de optimización? ¿No hace el problema más difícil?}
\end{quotation}
Si bien es verdad que hace el problema considerablemente más difícil, la verdadera ganancia es \emph{la generalidad del planteamiento}. En efecto, el promedio utiliza nociones vectoriales (sumas y ponderación), mientras que el problema de optimización definido en \eqref{eq:baricentro-problema-optimizacion} utiliza únicamente nociones métricas (distancias), la cual es una propiedad mucho más general. Esto hace que \emph{el baricentro depende únicamente de la métrica que se esté utilizando, siendo posible que este cambie dependiendo de la métrica}.

Ahora que se ha explicado el concepto de \emph{baricentro}, se pasará a explicar la parte de \emph{Wasserstein} del nombre. Consideremos la situación en la que se tiene una montaña de tierra, y se desea mover esta tierra a un pozo, el cuál posee un volumen igual a la de la montaña. La pregunta es: ¿Cuál es la manera de mover la tierra de la montaña al pozo de la manera más eficiente? La respuesta a esta pregunta la da la teoría de \emph{transporte óptimo}, y a partir de esta rama es que se define la \emph{distancia de Wasserstein}.

Sin entrar en muchos detalles, la distancia de \emph{Wasserstein} es una métrica entre dos medidas de probabilidad: una medida $\mu\in \ProbSpace[\cX]$ que representa la montaña, y otra medida $\nu\in \ProbSpace[\cX]$ que representa el pozo. Intuitivamente, la distancia de Wasserstein \emph{determina el esfuerzo promedio mínimo} que se necesita para mover la tierra de la montaña al pozo.

Con esta idea en mente, si es que la montaña estuviera más lejos del pozo, entonces la distancia de Wasserstein sería mayor. La propiedad de que esta distancia sea sensible a traslaciones espaciales (que muchas veces no la cumplen otras métricas y divergencias) le provee a esta distancia muchas propiedades interesantes.

Son estas propiedades que convierten a la distancia de Wasserstein en la candidata ideal para calcular baricentros. En donde, si se considera un conjunto de medidas de probabilidad $\{\mu_1, \ldots, \mu_n\}$, y denotando a la $p$-distancia de Wasserstein\footnote{La constante $p$ parametriza la distancia de forma similar a las $p$-normas, aunque para esta introducción, no es necesario entrar en detalles.} entre dos medidas $\mu$ y $\nu$ por $\Wasserstein[p]{\mu}{\nu}$, entonces el \emph{baricentro de Wasserstein} se define por
\begin{equation}\label{intro-wasserstein-barycenter}
    \bar\mu \eqdef \argmin_{\mu \in \ProbSpace[\cX]} \sum_{i=1}^n \gamma_i \Wasserstein[p]{\mu}{\mu_i}^p.
\end{equation}
donde $\left\{ \gamma_1, \ldots, \gamma_n \right\}$ es una secuencia de pesos no negativos que suman 1.

Con esta definición, el lector se podría preguntar:
\begin{quotation}
    \textit{¿Qué sucedería si en vez de tener un conjunto \textbf{finito} de medidas $\mu_1, \ldots, \mu_n$, tuviéramos un conjunto \textbf{infinito} de medidas?}
\end{quotation}
la respuesta es que, en ese caso, se ha de considerar una medida de probabilidad sobre medidas de probabilidad: una medida $\Gamma \in \ProbSpace[\ProbSpace[\cX]]$, que cumple el rol de los pesos $\gamma_i$ en la ecuación \eqref{intro-wasserstein-barycenter}. De esta manera, la sumatoria se convierte en una integral, y la definición del baricentro de Wasserstein se generaliza a
\begin{equation}
    \bar\mu = \argmin_{\mu \in \ProbSpace[\cX]} \int_{\ProbSpace[\cX]} \Wasserstein[p]{\mu}{\nu}^p \; {\Gamma(\dd \nu)}.
\end{equation}

Ahora que ya se conoce el concepto del \emph{baricentro de Wasserstein}, se procederá a explicar la componente \emph{Bayesiana}. En el enfoque Bayesiano, es usual llamar por \emph{modelos} a las de medidas de probabilidad, donde el conjunto de modelos se le denota por $\Models$.\FM{Podría dividir el parrafo aquí.}  Supongamos entonces que tenemos una muestra $\left\{ x_1, \ldots, x_n \right\}$ de $n$ datos que pertenecen a un espacio $\cX$.
% que son idéntica\FM{¿Está bien el tilde?}  e independientemente distribuidos de algún modelo $\tilde\mu$ desconocido.
A partir de esta muestra, se desea determinar un modelo $\bar{\mu}$ en $\Models\subseteq \ProbSpace[\cX]$ que mejor explique los datos, como si estos datos hubieran sido generados por $\bar{\mu}$.\FM{No me gusta cómo quedó esta frase, pero la mantendré por mientras.}

Para esto, se considera la verosimilitud de un modelo $\mu$ y que lo denotaremos por $\LikelihoodN[\mu]$, y un prior sobre los modelos $\Pi(\dd \mu)$. En virtud de la regla de Bayes, la \emph{medida posterior} se define por
\begin{equation}
    \Pi_n(\dd \mu) \eqdef \frac{\LikelihoodN[\mu]}{\int_{\ProbSpace[\cX]} \LikelihoodN[\nu] \Pi(\dd \nu)} \Pi(\dd \mu).
\end{equation}
de esta manera, la medida posterior le dará mayor pesos a aquellas medidas que sean más verosímiles con respecto a los datos. Así, el \emph{baricentro de Wasserstein Bayesiano} se define utilizando la medida posterior:
\begin{equation}
    \bar\mu \eqdef \argmin_{\mu \in \Models} \int_{\ProbSpace[\cX]} \Wasserstein[p]{\mu}{\nu}^p \; {\Pi_n(\dd \nu)}.
\end{equation}


Cabe mencionar que, gracias a las grandes propiedades que posee la distancia de Wasserstein y el baricentro de Wasserstein, es que ha sido utilizada en una gran variedad de aplicaciones:
para comparar los histogramas de colores \cite{rubner1998metric}, para comparar imágenes \cite{peleg1989unified}, para restaurar imágenes \cite{lellmann2014imaging}, para sintetizar texturas \cite{tartavel2016wasserstein}, en la clasificación de texto \cite{kusner2015word} y en particular,
en la función de pérdida para las redes generativas adversarias, para aliviar los problemas del desvanecimiento del gradiente y del modo de colapso \cite{arjovsky2017wasserstein}. Este tema se tratará en mayor profundidad más adelante en la tesis.\FM{Me causa ruido que se incluya este parrafo de esta manera.}

Sin embargo, desde la concepción del baricentro de Wasserstein Bayesiano, tan sólo se realizaron algunos experimentos con conjuntos de datos ``de juguete'' (utilizando distribuciones normales, por ejemplo) para estudiar sus propiedades matemáticas y estadísticas, pero no se han visto aplicaciones en conjuntos de datos más complejos (como podría ser en imágenes).

Una razón de esto, es porque obtener la distribución posterior $\Pi_n(\dd \mu)$ puede ser infactible de calcular para conjuntos de datos finitos, incluso si estos son muy grandes. Otro motivo es que, en el caso en que se utiliza un conjunto infinito de modelos, es necesario utilizar el Descenso del Gradiente Estocástico sobre el Espacio de Wasserstein \cite{backhoff2022stochastic}, el cuál es un algoritmo que, de la misma manera, es bastante reciente y no existen herramientas computacionales que la implementen de forma general, y mucho menos para conjuntos de datos complejos.

Por este motivo, el objetivo general de este trabajo de tesis es la implementación eficiente de una librería en \emph{Python} para el cálculo del baricentro de Wasserstein Bayesiano sobre conjuntos de imágenes, utilizando el Descenso del Gradiente Estocástico sobre el Espacio de Wasserstein.

Dicho esto, los objetivos específicos son:
\begin{itemize}
    \item \textbf{OE1}: Obtener aproximaciones de las medidas de probabilidad $\Gamma \in \ProbSpace[\ProbSpace[\cX]]$.
    \item \textbf{OE2}: Implementar el Descenso del Gradiente Estocástico sobre el Espacio de Wasserstein para una medida $\Gamma \in \ProbSpace[\ProbSpace[\cX]]$.
    \item \textbf{OE3}: Aproximar la medida posterior $\Pi_n(\dd \mu)$.
    \item \textbf{OE4}: Calcular el baricentro de Wasserstein Bayesiano.
\end{itemize}