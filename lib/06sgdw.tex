\section{Descenso del Gradiente Estocástico sobre el Espacio de Wasserstein}\label{sec:sgdw}  % MARK: SGDW

\FM[inline]{Podría ser de título ``Implementación del SGDW''? ya lo definí anteriormente}

En esta sección se presenta la implementación del SGDW.

\subsection{Interpretación de una Imagen como Medida}\label{ssec:interpr-imagen-medida}  % MARK: - Interpretación de una Imagen como Medida

Recordar que si $\mu\in \ProbSpace[\cX] $ es una medida discreta, entonces esta queda definida de la siguiente forma:
\begin{equation}\label{eq:medida-discreta}
    \mu = \sum_{i=1}^{n} m_i \delta_{x_i},
\end{equation}
donde $m \in \Simplex[n]$ es un vector de probabilidad y $ \left\{ x_1, \dots, x_n \right\} \subseteq \cX $ son sus posiciones. En el caso de una imagen, se puede interpretar como una medida discreta, donde $x_i$ es el $i$-ésimo píxel y $m_i$ es la intensidad de la imagen en el $i$-ésimo píxel. Por lo tanto, se puede muestrear un píxel de una imagen de la misma forma que se muestrea una medida discreta.

\FM[inline]{Es posible que este párrafo este de más.}

Cabe destacar que por definición, realizar un muestreo $x \sim \mu$ es equivalente a muestrear un índice $i \sim \Categorical(m)$ y retornar $x = x_i$. Por este motivo, se implementan las medidas discretas siendo extendidas de la clase \texttt{Categorical} del módulo \texttt{torch.distributions} de \textit{PyTorch}. Del mismo modo, como una imagen es una medida discreta, se extiende la clase para una medida discreta, tal que sea eficiente en memoria y en tiempo de construcción.

\subsection{Implementación del Algoritmo}\label{ssec:implementacion-algoritmo}  % MARK: - Implementación del Algoritmo

Dado que el Algoritmo~\ref{alg:sgdw-clasico} está diseñado para medidas absolutamente continuas, se reinterpreta este algoritmo para que se pueda trabajar con medidas discretas. Para ello, se destaca que la Definición~\ref{def:sgdw} se puede reinterpretar como la $\eta_k$-interpolación geodésica entre las medidas $\mu_k$ y $\tilde \mu_k$, mientras que la Definición~\ref{def:bsgdw} se puede reinterpretar como el baricentro de las medidas $\qty( \mu_k, \tilde\mu_k^{(1)}, \dots, \tilde\mu_k^{(S_k)} )$ con pesos $\qty(1-\eta_k, \frac{\eta_k}{S_k}, \dots, \frac{\eta_k}{S_k}) \in \Simplex[S_k+1]$. De este modo, el Algoritmo~\ref{alg:sgdw-clasico} se extiende de la siguiente manera:
\begin{algorithm}[H]
    \caption{SGDW General}
    \label{alg:sgdw-general}
    \begin{algorithmic}[1]
        \Require Acceso a las muestras de $\Gamma(\dd \mu) \in \ProbSpace[\ProbSpace]$, un esquema de paso $(\eta_k)_k \in [0, 1]^\N$ y un esquema de paso $(S_k)_k \in \N^\N$.
        \State{$k\gets0$}
        \State{Muestrear $\mu_0 \sim \Gamma$}
        \Repeat
        \State{Muestrear $\tilde \mu_k^{(1)}, \dots, \tilde \mu_k^{(S_k)} \simiid \Gamma$}
        \State{$\gamma\gets\qty(1-\eta_k, \frac{\eta_k}{S_k}, \dots, \frac{\eta_k}{S_k})$}
        \State Definir $\mu_k$ como el baricentro de $\qty( \mu_k, \tilde\mu_k^{(1)}, \dots, \tilde\mu_k^{(S_k)} )$ con pesos $\gamma$.
        \State{$k\gets k+1$}
        \Until{un criterio de detención ha sido alcanzado.}
        \State\Return $\mu_k$
    \end{algorithmic}
\end{algorithm}

Como se está trabajando con imágenes, se puede aprovechar su estructurar para calcular una estimación de los baricentros de manera más eficiente, utilizando el algoritmos de Baricentros de Wasserstein Convolucionales \cite{solomon2015convolutional} o su versión Insesgada (\textit{Debiased} en inglés) \cite{janati2020debiased}. Sin embargo, el Algoritmo~\ref{alg:sgdw-general} es lo suficientemente general para ser aplicado a cualquier medida discreta, utilizando la estimación del algoritmo de Sinkhorn \cite{cuturi2013sinkhorn}, por ejemplo. Todos estos métodos de cálculo de baricentros se implementan de manera eficiente utilizando la librería de \textit{Python Optimal Transport} (POT) \cite{flamary2021pot}, donde además esta librería admite la paralelización de los cálculos por medio del GPGPU \cite{owens2008gpu}.

\FM[inline]{Mejorar esto utilizando la medida $\Prob_X$}

Para muestrear a partir de una medida $\Gamma$ a partir de un conjunto de datos $\left\{ \mu_i \right\}_{i=1}^{N} \subseteq \ProbSpace[\cX] $, se puede calcular la medida empírica $\hat \Gamma$ de la siguiente forma:
\begin{equation}\label{eq:medida-empirica}
    \hat \Gamma (\dd \mu) = \frac{1}{N} \sum_{i=1}^{N} \delta_{\mu_i} (\dd \mu).
\end{equation}
Esto corresponde muestrear una imagen cualquiera de forma equiprobable. Otra manera de obtener una $\Gamma$, es a través de un modelo generativo, de manera que muestrear una imagen correspondería a simplemente muestrear un ruido aleatorio $z \sim \Prob_Z$ y aplicar la función generadora $G_\theta(z)$.

\subsection{Resultados y Discusión}\label{ssec:sgdw-resultados-discusion}  % MARK: -- Resultados y Discusión

\FM[inline]{En esta sección se podría incluir el cálculo de un baricentro, tanto del dataset como de la GAN}

\subsection{Conclusiones}\label{ssec:sgdw-conclusiones}  % MARK: -- Conclusiones

\FM[inline]{Insertar aquí alguna conclusión}

\FM[inline]{Una conclusión que se me ocurre es como los dos baricentros, el del conjunto de datos y el de la GAN se parecen, algo que debería de pasar puesto que ambos están aproximando a alguna medida de referencia $\Prob_X$}

